var store = [{
        "title": "Changing in-line code color",
        "excerpt":"I was getting annoyed with the difficulty of seeing the code with `` (backtick) symbols in my blog posts. To change these colors I went into my _sass/minimal-mistakes/_base.scss file and changed this part of the code:   /* code */  tt, code, kbd, samp, pre {   font-family: $monospace; }   to:   /* code */  tt, code, kbd, samp, pre {   font-family: $monospace;   color: #CC5500;   // background: $link-color }   #CC5500 is a nice burnt oragne color. I also tried changing the background color, but this also affected triple-backtick code snippets.  ","categories": [],
        "tags": ["jekyll","minimal-mistakes","css"],
        "url": "/backtick-code-color/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Deploying github page to own domain",
        "excerpt":"I wanted something a little more professional than https://[USERNAME].github.io for my website name. Come on, https://bendirt.com is at least memorable!   I mostly just followed this amazing guide. I recommend checking out that whole four part series, although it uses Docker, which I didn’t touch for my site… Another thing for future Ben.   Let me summarize the steps:      Go to your favorite place to buy domains and find a good one. I bought mine for 12/year from google domains.   Add a file called CNAME (no extension) to your https://github.com/[USERNAME]/[USERNAME].github.io website directory. That file will contain the name of your domain (e.g., www.bendirt.com).   Go the DNS settings on your google domain page and add github’s servers and the repository to the “Resource records”:            Host name: [YOUR DOMAIN].[YOUR EXTENSION] (e.g., bendirt.com); Type: A; TTL: 1h; Data:                    185.199.108.153           185.199.109.153           185.199.110.153           185.199.111.153                       Host name: www.[YOUR DOMAIN].[YOUR EXTENSION] (e.g., www.bendirt.com); Type: CNAME; TTL: 1h; Data: [USERNAME].github.io           Go back to your repository “Settings” &gt; “Pages”, and you should see your domain as the deployment location!   You need to be patient for a bit, but once the domain is ready you can also check “Enforce HTTPS” to protect visitors to the website   If you are having problems, github has a good troubleshooting page.  ","categories": [],
        "tags": ["github-pages","domain-name"],
        "url": "/deploying-own-domain/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Getting started with a jekyll, minimal mistakes, and github pages",
        "excerpt":"This site was built with jekyll. In my own words, this is a static (as opposed to dynamic? I think so) website generator that is written in the programming language Ruby. Jekyll uses a defined file structure, similar to Ruby on Rails, where there are specific ways to name folders and files (e.g., _layouts/, _includes/, _data/navigation.yml).   The files are a combination of CSS, Markdown, and HTML. There are also control-flow embedded tags in Liquid, that work very similar to embedded Ruby, and give access to global variables like paginator and site. If you see a .md or .html file headed by ---, then you can put Liquid tags there, because this is frontmatter that jekyll is looking for. All sorts of things go in there like the page title, navigation links, and more. I could go on, but let’s get to the practical implementation.   Starting a site   The easiest way to learn jekyll is by following their simple tutorial. I just want to summarize the steps that I did to get up and running.   I opened a new VS Code project in a folder call my-website/. I previously installed Ruby and RubyGems on my local laptop (running Windows 10), so I was able to go to the PowerShell terminal in VS Code and run   &gt; gem install jekyll bundler   This installed jekyll (somewhere in my C:\\Ruby31-x64 directory – sidenote, this is similar to the way I’m used to conda installing Python packages). Then I ran:   &gt; bundle init   That created a blank Gemfile in the directory, and into that file I dropped:   gem \"jekyll\"   After this, back at the terminal I ran:   &gt; bundle   This installed jekyll, because that’s what bundle does! It looks in you Gemfile and installs all the gems you need for the project.   At this point, I was able to make a toy file index.html in the my-website/ root directory, and then run at the terminal:   &gt; jekyll serve --livereload   Jekyll is a static site generator! So when I run this command, it first automatically does a jekyll build, which creates a new directory _site/ containing all the pages to be published online as HTML.   With the _site/ built, jekyll serve now gives me access to a local port http://localhost:4000/ that I can navigate to in a new internet browser window, and --livereload will insure that most changes I make on VS Code are automatically shown in the browser! Some changes (like those to _config.yml) will need to have the server restarted to show them (i.e., a new jekyll build has to run).   Overall, we have a great environment with VS Code for editing files, and the browser for viewing our website. There’s so much more jekyll to learn.   Minimal Mistakes   Building up an entire website from scratch with jekyll would be a lot of work, especially for something asthetically pleasing. So I took a common shortcut, and used the minimal mistakes jekyll theme to get something decent up and running.   Basically, you get a whole bunch of page layouts and boilerplate code in the jekyll language and structure with this repository that you can do a ton of customization on. There’s a nice quick-start guide here that I used.   Getting Minimal Mistakes to Work on GitHub   At first, I naively tried to use the gem-based method to get the template running. That actually worked pretty well in VS Code in my local browser. Just adding this gem and running the build and server got it working locally. I was able to play around a lot and spin up a website.   BUT WAIT, since the location I was deploying the site (github pages, coming up below) is limited in its build-ability (see this SO conversation), my naive approach didn’t work.   Instead you need to follow here, and have a Gemfile that looks like:   source \"https://rubygems.org\"  gem \"github-pages\", group: :jekyll_plugins gem \"jekyll-include-cache\", group: :jekyll_plugins   You also need to:      Add jekyll-include-cache to the plugins array of your _config.yml   Run bundle in the directory to update teh bundled gems   Add remote_theme: \"mmistakes/minimal-mistakes\" to your _config.yml file. Remove any other theme: or remote_theme: entry   Proto Directory Structure   After a lot of fooling around, I ended up with a prototype for my website that basically looked like:   my-website ├── _data                      # data files for customizing the theme |  ├── navigation.yml          # main navigation links |  └── ui-text.yml             # text used throughout the theme's UI ├── _pages                     # a sub-directory with all pages on my site |  ├── 404.md                   |  ├── about.md                 |  ├── cv.md                    |  ├── publications.md          |  └── research.md              ├── assets |  ├── documents               # some documents I link to |  ├── images                  # images on my website |  └── videos                  # videos on my website ├── README.md                  ├── _config.yml                # site configuration ├── Gemfile                    # gem file dependencies └── Gemfile.lock               # gemlock file, you can actually delete this   The git commit with all of that is here.   BUT WAIT, where the heck is all of the other files that jekyll needs… assets, _layouts, _includes, and _sass are stored in the theme’s gem! So I don’t need to make these directories or files! If you would like to make changes, create the files and Jekyll will prefer your local copy (source).   Where to host?   If you’re like me, then you may have dove deep on spinning up a nice website at http://localhost:4000/ while running jekyll serve in VS Code. Now comes the question, where do I actually deploy my website online so other people can see it?   Basically, this comes down to a matter of getting the static _site/ directory hosted on a server somewhere.   I deployed my site with github pages, mostly because it’s free and you can find a lot of help online from people doing the same.   Setting up github   First I created a new repository on my github account called [USERNAME].github.io (for me that would be repo at https://github.com/bpurinton/bpurinton.github.io). I set it to Public and initialized it with no license or README.   Now, back in my nice website directory, I renamed the directory bpurinton.github.io to match my github repo. I also added a README file, but I left this mostly blank.   Since I had been running the static site built with jekyll serve (which runs jekyll build), I had the _site/ directory ready to go. But github pages runs its own build on the repo, so at this point I can actually delete the _site/ directory. Basically, I use _site/ when I’m in production mode in VS Code and using the local port browser to make updates. When it’s time to push stuff to github, I just delete this directory. There’s probably a better way to do this by preventing _site/ from pushing, but that’s something for future Ben.   At this point I ran the following git commands from the VS Code terminal in the website directory (because I have git installed and available at the command line):   # initialize the repository with a .git/ directory git init  # add all files in the directory git add .  # commit the files git commit -m \"first commit\"  # set the current branch (I think?) git branch -M main  # set the upstream push location git remote add origin https://github.com/bpurinton/bpurinton.github.io.git  # push my files up git push -u origin main   Deploying   Now we need to do a few things (always pushing the changes with new git commits):      In _config.yml set url : \"https://[USERNAME].github.io\" and set repository : \"[USERNAME]/[USERNAME].github.io\"   On the github repository, go to “Settings” &gt; “Pages” and set the “Source” &gt; “Deploy from branch”, “Branch” to main.   Make sure the page is deployed at https://[USERNAME].github.io   The deployment will be running in the “Actions” tab of the repository, and you can monitor it there (everytime you push a change to your github repo, this action will be run). If there are errors, the page won’t deploy, but if everything goes well, then you will see the confirmation and if you navigate to https://[USERNAME].github.io, then you will see your static website online!   ","categories": [],
        "tags": ["jekyll","github-pages","minimal-mistakes"],
        "url": "/fresh-jekyll/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "First Post!",
        "excerpt":"This blog is going to be a collection of notes to myself, hopefully some of these are useful to others. I will post about computer programming and mix that in with some Earth science, because, you guessed it, I &lt;3 Dirt.   After years of working off a forked copy of the minimal mistakes Jekyll theme for academic pages (here), I decided it was time to start from scratch. I had little understanding of website development in 2018 when I first setup my site. Now that I’m learning more about Ruby on Rails, HTML, CSS, and how to deploy a site, I’m ready to build up a new, simplified website.   ","categories": [],
        "tags": [],
        "url": "/hi/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Adding a `sitemap.xml` for SEO",
        "excerpt":"SEO stands for Search Engine Optimization. I want to improve how my site comes up when people search for it. As usual, I’m relying heavily on this wonderful guide.   Register on Google Search Console   The first thing I did was register my domain on the Google Search Console. That was easy, because I had already purchased the domain from Google Domains. The search console allows us to track our site and improve the search results.   Generate a sitemap   For the search console, we need a sitemap.xml that is just a map of all the URLs in our domain. I noticed the site.xml was in the _site/ folder of my local build, and was updated everytime I ran:   bundle exec jekyll build   That’s the handy command-line way to locally rebuild the website in the _site/ folder when I make changes. However, if I run:   bundle exec jekyll serve   (which first executes a build), then the sitemap.xml appears like:   &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;urlset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\" xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt; &lt;url&gt; &lt;loc&gt;http://localhost:4000/backtick-code-color/&lt;/loc&gt; &lt;lastmod&gt;2023-01-03T00:00:00-09:00&lt;/lastmod&gt; &lt;/url&gt; &lt;url&gt; &lt;loc&gt;http://localhost:4000/deploying-own-domain/&lt;/loc&gt; &lt;lastmod&gt;2023-01-03T00:00:00-09:00&lt;/lastmod&gt; &lt;/url&gt; &lt;url&gt; &lt;loc&gt;http://localhost:4000/fresh-jekyll/&lt;/loc&gt; &lt;lastmod&gt;2023-01-03T00:00:00-09:00&lt;/lastmod&gt; ...   All of the domain is at the base URL of http://localhost:4000/. That’s because Jekyll 3.3 overrides config site.url with url: http://localhost:4000 when running jekyll serve locally in development. If you want to avoid this behavior set JEKYLL_ENV=production to force the environment to production.   Oops! I didn’t want to mess with the JEKYLL_ENV variable, and I wanted to continue making local edits, so rather than this, I just ran a plain:   bundle exec jekyll build   And that gave me a nice _site/sitemap.xml file:   &lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt; &lt;urlset xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.sitemaps.org/schemas/sitemap/0.9 http://www.sitemaps.org/schemas/sitemap/0.9/sitemap.xsd\" xmlns=\"http://www.sitemaps.org/schemas/sitemap/0.9\"&gt; &lt;url&gt; &lt;loc&gt;https://bendirt.com/backtick-code-color/&lt;/loc&gt; &lt;lastmod&gt;2023-01-03T00:00:00-09:00&lt;/lastmod&gt; &lt;/url&gt; &lt;url&gt; &lt;loc&gt;https://bendirt.com/deploying-own-domain/&lt;/loc&gt; &lt;lastmod&gt;2023-01-03T00:00:00-09:00&lt;/lastmod&gt; &lt;/url&gt; &lt;url&gt; &lt;loc&gt;https://bendirt.com/fresh-jekyll/&lt;/loc&gt; &lt;lastmod&gt;2023-01-03T00:00:00-09:00&lt;/lastmod&gt; ...   Submit the sitemap   Now this is what I need to upload to the Google Search Console “Sitemap” at: https://search.google.com/search-console/sitemaps.   But I can’t just upload this, I need to submit a URL….   All that dancing around for nothing. If I visit my deployed website, then an updated sitemap.xml already exists at https://www.bendirt.com/sitemap.xml. I guess this was generated during the github-pages deployment?   Now it’s just about patience with the Google Search Console, since it may take a few days to update with analytics.  ","categories": [],
        "tags": ["github-pages","seo","jekyll"],
        "url": "/sitemap-and-SEO/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Writing a textbook with bookdown",
        "excerpt":"I am working on an outline for a textbook for a fullstack programming course. I want to break my outline into lessons and generate a nice HTML view from the markdown files that will become the basis for an interactive textbook. Bookdown is the tool, documentation can be found here. Here is how I set it up and got started.   Installing R with VS Code   Bookdown using the R programming langauge and markdown documents. So the first step is getting R running with the necessary packages. And I want it integrated in VSCode.      Install R: https://cloud.r-project.org/ (make sure to check “Save version number in registry”)   Install VSCode R editor support: https://marketplace.visualstudio.com/items?itemName=REditorSupport.r   In R install languageserver: https://github.com/REditorSupport/languageserver            open RGui on Windows       run install.packages(\"languageserver\")       also install rmarkdown with: install.packages(\"rmarkdown\")           Make sure R is on the system PATH, for me that was C:\\Program Files\\R\\R-4.2.2\\bin   Close and re-open VS Code if you cannot access R interactive shell from the command-line. There should also be a new terminal option “R Terminal” that opens an interactive shell.            can double check the packages installed with install.packages(\"languageserver\") at the R Terminal, this shouldn’t do anything.           Once that was setup, I made a test R script test.R:   # My first program in R Programming myString &lt;- \"Hello, World!\"  print (myString)   And ran R test.R at the terminal. It worked. So R is up and running!   First Steps with Bookdown   To install bookdown for R, I just need to run (at the R terminal):   install.packages(\"bookdown\")   Minimal Example   Bookdown is built on rmarkdown. A demo for staring a project can be found here. However, I used the documentation minimal example here.   render_book()   I downloaded index.Rmd, then in the R terminal I ran:   bookdown::render_book('index.Rmd', 'all')   This generated a folder _book/ with a static HTML version of the content. This is the built book, and all arguments can be found in the documentation:   bookdown::render_book(input = \".\", output_format = NULL, ..., clean = TRUE,   envir = parent.frame(), clean_envir = !interactive(),   output_dir = NULL, new_session = NA, preview = FALSE,   config_file = \"_bookdown.yml\")   By leaving output_format empty, the book is rendered as the first output format specified in the YAML metadata of the first .Rmd file or a separate YAML file _output.yml (see here). When you set preview = TRUE, only the Rmd files specified in the input argument are rendered, which can be convenient when previewing a certain chapter, since you do not recompile the whole book, but when publishing a book, this argument should certainly be set to FALSE.   clean_book()   You can delete the built book with:   bookdown::clean_book(TRUE)   But be careful that things are version controlled with git so you don’t lose anything.   Viewing the Book   To view the static site in the _book/ directory. I installed the VSCode extension Live Preview. All I need to do is select one of the .html files, click the preview button in the code editor, and there it is. I can also just navigate to http://127.0.0.1:3000/_book/ in my browser. It even updates as I add chapters and redo the render_book() command.   If the book is getting long and the pandoc conversion step of render_book() is taking awhile, then an individual chapter can be re-rendered using:   bookdown::preview_chapter(\"index.md\")   Practical Example   Chapter Order   My default, bookdown will render files in the order of their name (e.g., 01-something.Rmd, 02-something-else.Rmd, etc.). I don’t want to provide an order in filenames, because I may want to remix and rematch. So I just need to override this behavior by making a _bookdown.yml file in the book directory and providing a file list:   rmd_files: [\"02.Rmd\", \"01.Rmd\", \"index.Rmd\"]   Filenames that start with an _ underscore are skipped. The file index.Rmd will always be treated as the first even if re-ordered in the list.   I can create an appendix with # (APPENDIX) Appendix {-} at the top of a file, the {-} characters tell markdown not to give this a section number   Front Matter   For now, I want the book to render as a gitbook, which provides nice links and a navigation pane. So the front-matter of the index file should look like so:   # index.Rmd  --- title: \"A Book\" author: \"Ben\" site: bookdown::bookdown_site  documentclass: book output:   bookdown::gitbook: default   #bookdown::pdf_book: default ---   The site: bookdown::bookdown_site pair calls bookdown::render_book(), and the output is specified as bookdown::gitbook. The outputs can also be specified in a separate _output.yml document as shown here, which is maybe better from an organizational standpoint. So now if I just run:   bookdown::render_book()   then everything works. Also because I have the _bookdown.yml in my root directory, which render_book() looks for by default, my chapter order and other variables are set correctly.   Markdown   But what if I don’t care about .Rmd files, because I don’t plan to have integrated R code? Well, I can just use .md markdown files! If I changed all the filenames and replace the _bookdown.yml file list with:   # _bookdown.yml  rmd_files: [\"02.md\", \"01.md\", \"index.md\"]   And when I bookdown::render_book() and visit http://127.0.0.1:3000/_book/, then everything looks good!   For the # heading in each file, if I add {-}, then it will skip section numbering.   Publishing on GitHub   I want to create github repo for the project, and I would also like to view the rendered site as updates are made. There are instructions here.   Other useful stuff      you can add bibliographies and citations: https://bookdown.org/yihui/bookdown/citations.html   you can add HTML widgets: https://bookdown.org/yihui/bookdown/html-widgets.html   the _bookdown.yml file is highly customizable: https://bookdown.org/yihui/bookdown/configuration.html#configuration  ","categories": [],
        "tags": ["bookdown","markdown","r","html"],
        "url": "/bookdown/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Custom JavaScript in Bookdown",
        "excerpt":"I wanted some custom behavior in my bookdown project. Specifically, I wanted all the internal cross references to open in new tabs so that the reading flow is unbroken.   The Issue   With external references, bookdown’s formatting makes it easy to write:   [some website](theURL){target=\"_blank\"}   which parses to   &lt;a href=\"theURL\" target=\"_blank\"&gt;some website&lt;/a&gt;   But the bookdown internal cross-reference works like:   [some cross reference][the section ID]   This doesn’t accept the additional {target=\"_blank\"}. So how to fix this?   JavaScript to the Rescue   JavaScript &lt;script&gt;s in the HTML document allow you to modify the HTML after bookdown (well, pandoc actually) finishes rendering the output into HTML static files. My understanding is that this re-rendering and HTML addition is actually carried out by the browser when it encounters the &lt;script&gt;&lt;/script&gt; tags in an HTML document.   First, I found this post, and the first answer there (this one) told me that I could use a very simple JS script to retroactively add all my nice target=\"_blank\"s:   var links = document.getElementsByTagName('a'); var len = links.length;  for(var i=0; i&lt;len; i++) {    links[i].target = \"_blank\"; }   This script gets all the &lt;a&gt;&lt;/a&gt; link tags, and adds the code iteratively in a loop. If the link already had a target=\"_blank\" tag, it just overwrites it. Probably it would be “more correct” to ignore those that already have the tag, but it ain’t broke so I ain’t fixin it.   How do I get this thing to run?   I’m fairly new to JS, but luckily I found this post regarding JS and bookdown. Unfortunately the person was not answered, but their code told me I just needed to do three things in my bookdown project.   First, I needed to make a new file in the assets/styling/ folder called scripts.html. I could put this file anywhere and call it anything, but I consider what I’m doing here “styling”. Now any other JS scripts I want in my project can go into assets/styling/scripts.html.   Second, I just need to drop the above JS script into the new file with some HTML tags:   &lt;script&gt;   var links = document.getElementsByTagName('a');   var len = links.length;    for(var i=0; i&lt;len; i++)   {     links[i].target = \"_blank\";   } &lt;/script&gt;   Lastly, I need to point the file out to bookdown, so that it will add the script to each HTML file, which will then later be interpreted by the browser. I just need to add the following to the _output.yml file associated with my bookdown project:   bookdown::gitbook:   css: assets/styling/style.css   pandoc_args: [\"--lua-filter=assets/styling/footnote.lua\"]   includes:     in_header: assets/styling/style.html     after_body: assets/styling/scripts.html   # ...   Now the project includes my custom script added to every HTML page.  ","categories": [],
        "tags": ["bookdown","markdown","html","javascript"],
        "url": "/javascript-in-bookdown/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Setting up Ruby on Rails with PostgreSQL",
        "excerpt":"I just spent a long time getting my Windows machine setup with Ruby on Rails and the PostgreSQL database in order to work on some software development. Here are the steps I followed.   Docker + Ruby on Rails   Ideally, I could have set things up as a docker container, with the steps here, but that didn’t work at all. I had a lot of issues with docker on my machine, which may have come down to my own misunderstanding, or may have had to do with some similar bugs reported here or here or maybe here. In any case, I took a different approach.   Install Linux Subsystem   The first step was getting a Linux subsystem installed on Windows. I mostly followed the steps here. But I had some diversions due to issues. In the end my steps were as follows:   Open a Windows PowerShell as an admin and run   dism.exe /online /enable-feature /featurename:Microsoft-Windows-Subsystem-Linux /all /norestart   to enable the WSL subsystem feature.   Then run   dism.exe /online /enable-feature /featurename:VirtualMachinePlatform /all /norestart   to enable the virtual machine feature.   I then needed to download and run the WSL2 (version 2!) kernel updater shown in step 4 at the link above.   After that update, back in the admin PowerShell, I ran:   wsl.exe --set-default-version 2   (Note: in many cases I had to add an .exe to my commands, because e.g., wsl &lt;some command&gt; was not working.)   On the Microsoft store, I installed Ubuntu-20.04.2 LTS. Now I can open an Ubuntu shell on my machine!   Aside: Getting Ubuntu Shell Looking Nice   I added to my .bashrc (found in the Ubuntu home directory ~/.bashrc) the following:   parse_git_branch() {  git branch 2&gt; /dev/null | sed -e '/^[^*]/d' -e 's/* \\(.*\\)/(\\1)/' } if [ \"$color_prompt\" = yes ]; then  PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[01;31m\\] $(parse_git_branch)\\[\\033[00m\\]\\$ ' else  PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w$(parse_git_branch)\\$ ' fi  # THE SIX LINES BELOW are the default prompt and the unset (which were in the original .bashrc) #if [ \"$color_prompt\" = yes ]; then #    PS1='${debian_chroot:+($debian_chroot)}\\[\\033[01;32m\\]\\u@\\h\\[\\033[00m\\]:\\[\\033[01;34m\\]\\w\\[\\033[00m\\]\\$ ' #else #    PS1='${debian_chroot:+($debian_chroot)}\\u@\\h:\\w\\$ ' #fi #unset color_prompt force_color_prompt   Basically, I commented out the default behavior and added the git branch parser so I can see what git branch I am on in the shell (if I’m in a repo folder).   There’s a lot more you can do in the ~/.bashrc file to customize your shell. Like you can set what commands are run when the shell opens, because this file is run everytime you open a new shell.   Setting up Ruby on Rails   Now that I have a Linux virtual machine on my PC, I followed these steps to get Ruby on Rails running. There were again some diversions due to errors, but the general steps (and commands) were:   Ruby   In the Ubuntu WSL shell, run:   sudo apt-get update sudo apt-get install git-core curl zlib1g-dev build-essential libssl-dev libreadline-dev libyaml-dev libsqlite3-dev sqlite3 libxml2-dev libxslt1-dev libcurl4-openssl-dev software-properties-common libffi-dev   These are some dependencies. Once that’s done we need a Ruby version manager called rbenv. That is installed with these commands:   cd git clone https://github.com/rbenv/rbenv.git ~/.rbenv echo 'export PATH=\"$HOME/.rbenv/bin:$PATH\"' &gt;&gt; ~/.bashrc echo 'eval \"$(rbenv init -)\"' &gt;&gt; ~/.bashrc exec $SHELL  git clone https://github.com/rbenv/ruby-build.git ~/.rbenv/plugins/ruby-build echo 'export PATH=\"$HOME/.rbenv/plugins/ruby-build/bin:$PATH\"' &gt;&gt; ~/.bashrc exec $SHELL   (Note the echo ... commands send some copy into the /.bashrc via the &gt;&gt; shovel operator. Now this code will be run whenever the shell opens or whenever we run exec $SHELL.)   We now want to install Ruby with the manager. For my current project, I needed version 3.1.2, so I installed that at the Ubuntu shell with:   rbenv install 3.1.2   Then set it to be the global Ruby version for the shell with:   rbenv global 3.1.2   Now whenever I want to install or switch versions in a shell I can just do   rbenv install &lt;version&gt; rbenv global &lt;version&gt;   And confirm the version with   ruby -v   We also need the all-important bundler gem, which is a Ruby environment manager (kinda like conda for python?). In the Ubuntu shell I ran:   gem install bundler rbenv rehash   Git   At the Ubuntu shell I ran:   git config --global color.ui true git config --global user.name \"YOUR NAME\" git config --global user.email \"YOUR@EMAIL.com\" ssh-keygen -t rsa -b 4096 -C \"YOUR@EMAIL.com\"   The first three commands are just for setting some colors and my username on Github “Ben Purinton” and email. The last ssh-keygen command generates a public/private SSH key. The last command asks for a location (I used the default ~/.ssh/id_rsa.pub) and password ([redacted]).   I put the public key on my Github profile by pasting the output of:   cat ~/.ssh/id_rsa.pub   to the relevant area of my Github account (“Settings” &gt; “SSH”).   Now I don’t need to go and enter my Github password all the time! I can check it’s working by running ssh -T git@github.com, which gives back some “you’re connected” copy.   Rails   To install Rails, at the Ubuntu shell I ran:   curl -sL https://deb.nodesource.com/setup_12.x | sudo -E bash - curl -sS https://dl.yarnpkg.com/debian/pubkey.gpg | sudo apt-key add - echo \"deb https://dl.yarnpkg.com/debian/ stable main\" | sudo tee /etc/apt/sources.list.d/yarn.list  sudo apt update sudo apt-get install -y nodejs yarn   These are to cover dependencies for Rails. NodeJS is a Javascript runtime and Yarn is a package manager.   Now I can install Rails (which is just a gem!) by running:   gem install rails -v 7.0.4   The version can of course be changed, but I just used the latest version (7.0.4).   I also need to rehash via:   rbenv rehash   And confirm the install via:   rails -v   PostgreSQL   Now came the really annoying part. I’ll spare all the gory details, but here’s what worked for me. I began with the steps here, but did NOT create a new user. My commands were:   sudo apt-get remove postgresql   To remove any previously installed package. (Note: if you really want to purge postgresql then you need to run sudo apt-get --purge remove postgresql*).   Then I installed the database and its dependencies with:   sudo apt install postgresql postgresql-contrib libpq-dev   Now it should work! You just need to type:   sudo service postgresql start   To start the database. You can stop it by changing start to stop   You can also start a database with sudo pg_createcluster 14 main (changing 14) to whatever version you have installed, which you can check by doing an ls /etc/postgresql where you should see a folder with the version number. Actually, in that folder you will find the named databases (e.g., main). Oh and while I’m at it with these run on thoughts, when things are really messed up you can also drop a database wtih pg_dropcluster… Oh, and by the way, the database seems to exist at both /etc/postgresql/14/main/ AND /var/lib/postgresql/14/main/, go figure… TLDR; follow the steps / commands above and below and hopefully you don’t need to worry about all these asides.   Everytime we start a shell, we need to run:   sudo service postgresql start   But if you forget, when you go and start your Rails app, you’ll immediatelly get an error connecting to ActiveRecord, so that’ll be your reminder to start the database. You could also add the command to ~/.bashrc.   How to fix postgres user error   Thought I was done? Me too! Nope! When I tried to bin/setup or rails server in my Rails app that I wanted to work on, I got the error:   ActiveRecord::DatabaseConnectionError: There is an issue connecting to your database with you username/password, username: postgres.   When you install postgres I guess it generates a superuser named postgres that has full control over databases you create. But the Rails app I was working in said there was some error with this profile / password combo (that I had no part in setting up…)   Luckily, a colleague pointed me towards this post, which I summarize:   Running (psql is a terminal-based front-end to PostgreSQL):   psql -U postgres   Gave me the error message:   psql: error: FATAL: Peer authentication failed for user \"postgres\"   Uh-oh! I need to configure a password!   First I ran:   sudo -i -u postgres   That opened a sub-shell as the postgres user (as opposed to the username I used to setup my Ubuntu subsystem: bpurinton). Now at this new sub-shell prompot I ran:   psql   which opens the PostgreSQL terminal, where the prompt starts with postgres=# and you enter commands after that #. Now I ran:   ALTER USER postgres PASSWORD 'postgres';   To give the postgres superuser a password (in quotation marks)! The above returns ALTER ROLE. Great, thanks for letting me know. Now I can exit the postgres terminal by typing \\q at the # prompt, then exit the postgres user subshell by typing exit, and wind up back at my standard Ubuntu shell.   Another step! I need to alter the pg_hba.config file to allow access. Using an editor (I just did it with nano):   sudo nano /etc/postgresql/14/main/pg_hba.conf   14 and main would need to be changed if you have a different postgres version or cluster name. Basically, the file says, somewhere near the bottom this:      And I needed to change all the peer keywords to something like scram-sha-256, like so:      Now, with the file changed and saved, back at the Ubuntu shell I run:   sudo service postgresql restart   then:   psql -U postgres   and I don’t get any errors and now I’m good to go! bin/setup and rails server work. Finally!  ","categories": [],
        "tags": ["postgresql","ruby","rails","wsl"],
        "url": "/ruby-on-rails-with-postgres/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Git merging main into an in-progress feature-branch",
        "excerpt":"In a collaborative (or solo) git project I’m often in a position where I was working on a feature-branch, got busy, stopped working on it, then came back to it awhile later. In that time, there were changes on main (formerly master), that might affect my feature branch.   The question is: how do I merge the changes on main into my feature-branch, such that, when I merge the feature-branch into main down the road, there is a clean history?   We could merge --squash, but what we really want for a nice clean history is explained here.   Note, this blog post is a companion to this longer post   Change the editor   Before beginning, it’s nice to change the editor that Github uses away from VIM, which I find to be a pain. On my macbook, I ran:   $ git config --global core.editor \"code --wait\"`   to change the default editor to VS code. The --wait option freezes the command line until I close the file in the editor.   Getting main up to date   First, I pull main locally:   $ git checkout main (main) $ git pull   Once everything is up to date there, I switch back to my branch:   (main) $ git checkout feature-branch (feature-branch) $   Source vs. Target Branch   Quick note here. We are trying to merge like main --&gt; feature-branch. The branch with added changes we are trying to gather is called the source branch (here main) while the branch you request to merge your changes into is called the target branch (here feature-branch). We want to merge main (source) into feature-branch (target).   Rebase into feature-branch   Now I want to rebase the main into feature-branch.   (feature-branch) $ git rebase main   I could have added the -i (or --interactive) flag, like git rebase main -i. This would allow me to craft a commit history by squashing all the commits except for one that I pick (the top-most in the editor). picking multiple is almost never worth it (or, alternatively, it means your branches are too long lived). We skip the -i flag at this stage because we don’t need to make the history clean, we just need to get all the changes from main. Later, when we merge the feature to main, we will use -i.   After the rebase, I need to pull to get all the changes:   (feature-branch) $ git pull   If an error comes up regarding divergent branches, then we need to specify how to reconcile them. We can pass one of the flags --rebase, --no-rebase, or --ff-only. We need to use --rebase because --ff-only will fail since the local and remote branches have diverged:   (feature-branch) $ git pull --rebase   Now I can push:   (feature-branch) $ git push   These two actions take all of the main commits that occurred since making the feature branch and slots them into the feature branch’s history   And if I check on the open PR for the feature branch, I will see all of the history from main, and I can continue working on (and making commits) the feature branch.   Merge to main   When I’m done with the feature-branch (meaning I’ve made all the commits I’m going to and pushed them all to Github) and ready to merge to main, I can      merge on Github using the open PR, including receiving code review and then clicking “Squash and merge”   OR if I want to merge without the PR / code review:      run these commands:   git checkout main git pull git checkout feature-branch git rebase main -i   At this point, an editor will pop up. I can replace pick for all but the first commit (top-most in the editor) with s (for squash). Then I save and close the file.   Another text editor will open where you can craft a wonderful commit message to communicate the WHY of your changes (the WHAT is told by the diff).   Now, with the messy history of commits ironed out, we merge the feature to main:   git checkout main git merge feature-branch git push   And if we want to delete the remote feature-branch branch from Github:   git push origin :feature-branch   And locally we can delete the branch with:   git branch -d feature-branch  ","categories": [],
        "tags": ["git","rebase"],
        "url": "/merging-feature-branch/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Notes on using git",
        "excerpt":"I’m currently going through the extremely helpful thoughtbot git tutorial. I want to leave some notes here for future reference.   Git Aliases   Before I get into the notes, here’s a bit about .gitconfig. This dot file is found in two locations:      ~/.gitconfig: the home directory for my terminal, where the global options are set   git-project-directory/.git/.config: the .git/ directory of a git repo, where the local options are set   You can add aliases and other things to the global or local configuration using the command line syntax   git config --global alias.p push   or (for a longer one):   git config --global alias.as \"!git add -A &amp;&amp; git stash\"   Do a more ~/.gitconfig to view the changes, or even add more complex ones by opening the file in an editor.   At any time, I can remind myself what an alias is actually doing by running, e.g.:   git help as   Getting Confident   The first video covers some basics to get us feeling comfortable.   It starts by introducing the look of the git command line.   Git Context at prompt   A good terminal prompt will show:      whether you are in a git repo   what branch you are on in that git repo   a “dirty” status symbol if there are changed and un-committed   I use iTerm2 combined with oh my zsh, which gives my terminal a great look and feel:      The branch is (main) and there’s a little x symbol next to the prompt when I have un-committed files.   Git Stash   git stash is the command to “stash” any changes since the last commit that you might want, or might not, and you just want to save them somewhere. Eventually the stash is deleted so this is just a temporary holding area.   I have the alias git as setup, which first adds all untracked files and then stashes them (stash only takes tracked files by default).   Reflog   The git reflog command is great, it lists every command we ran in a repo that changed what we have checked out: every commit, branch, rebase, etc.   Writing:   git reflog branch-name   shows us just the actions on that specific branch:      Note on colors      yellow = commit hash; this is the identifier for this state of the code   blue = HEAD; this indicates where I am currently checked out with the -&gt; symbol pointing at a local or remote branch that I am currently on   green = local branch; this is a branch on my local machine   red = remote branch; this is branch on Github   black = command or commit message; this is the text describing what the line refers to   reset --hard with reflog   I can use the reflog to reset my code to any previous state! This command moves the HEAD pointer to some previous commit with the hash 9454622:   git reset --hard 9454622   With that powerful command, we don’t need to worry about messing things up!   Viewing history   The second video covers how we view the history. There’s a bunch of things in there about viewing granular details of file changes and searching the history with grep. Good to know, but here are just a few important commands.   git log   or   git log --oneline --decorate   To view a linear history of the repository. I have this aliased as git sl.   git log --pretty=format:'%C(yellow)%h%C(reset) - %an [%C(green)%ar%C(reset)] %s'   To view the commits, who made them, when they made them (in green), and the one line summary. I have this aliased as git pl.   git show 0302t5   To show the commit information and a git diff of the changed files.   The git graph   This is important but confusing at time. Recall the color notes.   git log --oneline --decorate --graph --all   To view a graph showing the relation between branches. I have this aliased as git sla.      Here is a good SO answer on how to read the graph. Some notes:      The top is the most recent, the bottom is the oldest   Branches are | vertical lines   The * tells you which branch the commit is referring to.   Where two branches come together or converge (i.e.: |\\), that’s a merge. Where they split or diverge (i.e.: |/), that’s where the branch was created/taken off of.   Therefore, to read the development of a branch, start with where it splits off, read the log entries for any lines corresponding to a * in that branch’s vertical line, and follow it until it merges again. Note that not every log entry will be relevant, only those with a * on the branch in question. Sometimes branches are taken from far back, which means they have to ‘jump over’ a lot of commits when they’re merged.   Undoing   The third video covers how we undo mistakes.   My main takeaway here, is that we can use git add file-name to add an unstaged file and then add that file to the previous commit (where maybe we left it out accidentally) with:   Ammend commit   git commit --amend --no-edit   or   git commit --amend   which will allow us to actually edit the previous commit message.   Unstage a file   We can also un-stage a file (maybe because it doesn’t make sense to include with the commit because it is unrelated to the other files!) with:   git reset file-name   I have this aliased to git unstage, because that name makes more sense.   git status   is a command you should run a lot to double check what you’re doing:      A dangerous checkout   A dangerous command is:   git checkout .   This will back out all of your changes and reset you to the previous commit. If you haven’t staged or committed the changed files, this will destroy them.   reset --soft to undo a commit   We can remove a commit from our history and reset the files to their staged state with all of the changes we made with:   git reset --soft HEAD^      HEAD refers to the current branch that you are checked out on   ^ means one commit back (the “parent”)   soft means leave the files in the working directory and index untouched   Then we could add some more changes to the files and commit again. I have this useful command aliased as git uncommit.   Steps to completely backout of a commit   First git uncommit to re-stage the files, but undo the commit. Then git unstage (optionally providing a filename we want to unstage, or taking them all by default), to unstage the files. Then, the dangerous git checkout ., which will completely reset our code and history to how it looked before our most recent edits and commit.   Crafting history with rebase   The fourth video covers rebase. There’s a lot in there, but these are some notes on my important takeaways.   The way we want to be working is via commits on a local feature branch, then a merge to main when it’s ready.   patch and cherry-pick   We use git add --patch (rather than git add .) to selectively stage granular changes within files that we change. This will lead to a partially staged and unstaged file. We can commit, then stage again, then commit again, which will break up the commit based on how we selectively staged with --patch.   We use git diff to view the file difference in unstaged changes, or git diff --cached for the staged changes.   If we accidentally commited to main when we meant to be on a feature branch, we can use:   git diff 23532..12352   This will show differences in the files in the range specified between the two hashes with the .. notation. We use that to check which commits we want to move.   We can then checkout the feature branch: git checkout feature-branch and run:   git cherry-pick 23532..12352   This will move all the specified commits (we can also just pass a single commit rather than a range) to the feature branch. This will create new commits on the branch, it doesn’t just pick them up and move them. So we will still see those commits in main’s history. But, we know how to reset main and cleanup the history with a hard reset.   rebase   A rebase is pretty much identical to the cherry-pick workflow! In this case, we have some stuff (commits) on main and we want to move them into the feature-branch.      Here, I have the commit 7ea8c45 on my local (it’s green) main branch, but I want to move this commit and the changes associated with it into my new-git-post branch.   The first step is to checkout the feature branch. In my example, I have checked out becuase HEAD -&gt; new-git-post.   With rebase, we want to take the work we’ve done on our feature branch (which was new-git-post here), and reapply it as if it was done on top of the additional commits in our target branch (which was main here).   Now, I’m going to make couple changes to new-git-post. Then I realize, oh, I actually would also like the change on the main branch.      So we have b07c153 to 3619d72, which are the commits on the feature branch that I’m checked out on, and 7ea8c45, which was the commit on main that I want in the currently checked out feature branch. Now I can run:   git rebase main   And the resulting history:      We see that the commit hashes have changed, but now the history appears that the current feature branch is a continuation of main, including the 7ea8c45 commit that we wanted to grab.   Interactive rebase   When we want to really craft the history and make the changes clear (like when we’re getting ready to merge to main), then we want to use rebase -i for interactive rebasing.   This won’t move commits, but rather revises commits in place. We use it to combine or squash commits.   We never want to revise the history like this on main, but we do want to revise the history of feature branches before merging them to main   Let’s say we wanted to take the top few commits and squash them down into one commit. We would do   git rebase main -i   This would allow me to craft a commit history by squashing all the commits except for one that I pick (the top-most in the editor). picking multiple is almost never worth it (or, alternatively, it means your branches are too long lived).   When I run the command, an editor pops up:   pick bf9674d WIP on new git post pick 46c743f added rebase-01 image and WIP pick 473aea8 WIP on post pick 27cd357 more WIP pick 22d9e46 added more images and continued WIP pick 0afae16 WIP on interactive rebase pick d105127 WIP   I edit that to look like:   pick bf9674d WIP on new git post s 46c743f added rebase-01 image and WIP s 473aea8 WIP on post s 27cd357 more WIP s 22d9e46 added more images and continued WIP s 0afae16 WIP on interactive rebase s d105127 WIP   This will squash the subsequent commits after the first commit on the branch (top-most in the editor) down into the first commit. We could pick others if there were important ones that stood out.   We get another editor when we close the file, where we wrote a longer description with all of the information about what happened up to the current point in time after the first commit that we picked:   I'm just squashing some commits to demonstrate how to interactively rebase.  # This is a combination of 7 commits. # This is the 1st commit message:  WIP on new git post  # This is the commit message #2:  added rebase-01 image and WIP   (Note: we could have even deleted all of the commit messages from each individual commit below this note we added.)   And when we close that, our history might look something like:      And we could continue working on our branch. But usually, these would be the steps just before we merge to main to create a cleaner history.   Interactive rebasing does not modify the code, it just modifies the commit history!   Merging to main   This isn’t in the videos, but just references one of the other posts on my blog. When the interactive rebase is done, and the history is cleaned, and we’re ready to merge to main, we can run:   git checkout main git merge feature-branch git push   And we can even delete the feature branch with   git branch -d feature-branch  ","categories": [],
        "tags": ["git"],
        "url": "/notes-on-git/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Running dockerized google earth engine (GEE) map",
        "excerpt":"The steps I took to open and run a dockerized geemap:      Install docker and learn some useful commands   Gather the geemap image from https://hub.docker.com/r/bkavlak/geemap with a docker pull because it seemed like the most updated. I did not follow the instructions for staring things there exactly. I also used a nice youtube video from Simon Mudd. So the following instructions are somewhat bespoke.   Make a local directory ~/geemap-data which will be the bridge between the container running the application and my local computer   Start the docker container based on the image by running:      docker run -it --name geemap -p 8888:8888 -v ~/geemap-data:/geemap/data bkavlak/geemap:latest           This will put me inside the container, where I should see the data directory that is connected to my ~/geemap-data local folder.   Start a jupyter environment by running:      jupyter lab --ip=0.0.0.0 --port=8888 --allow-root           Open localhost:8888 and copy-paste the unique token=... into the browser.   Start a notebook and run ee.Authenticate() to go through the pipeline of getting an authentication token. This needs to be done everytime I open the container. There’s probably a way around this but it’s not a huge deal to re-authenticate when I’m starting work for the day.   Work away and save everything into data/ (which will pipe it to my local ~/geemap-data folder).   When I exit the container, I also need to delete it or it won’t open next time due to the name being taken. I have tried to just re-open the container, but I’m having problems there. Again, not ideal but it works for now! This is how I drop the container (the image stays though so it makes for quick rebooting from step 4 above):     docker ps -a docker rm -f &lt;container-ID&gt;           Now I can make some sweet maps:      Some more notes on docker   On my M2 Mac with arm64 Apple Silicon architecture, I need to build images by adding FROM --platform=linux/amd64 &lt;parent-image&gt; to the top of the Dockerfile.   Then I can build as usual with docker build -t &lt;image-name&gt; .   Then when I want to run an image locally I need to supply the platform with docker run -it --platform linux/amd64 &lt;image-name&gt;  ","categories": [],
        "tags": ["docker","geemap","python","jupyter"],
        "url": "/docker-geemap/",
        "teaser": "/assets/images/favicon-32x32.png"
      },{
        "title": "Exploring Nome's Sea Ice with Satellite and Ground Radar",
        "excerpt":"I recently found out about UAF’s sea-ice radar monitor at the end of the pier in Nome’s port.      Location of radar in port of Nome. Image from here.       Latest radar image from the port.    Naturally, I wanted to investigate how this ground-based radar compares with satellite observations of recent sea ice conditions. Here’s what I found out.   Quick radar background   Radar sensors measure light (a.k.a. radiation) reflected off of surfaces. Just like a human eye; however the wavelength of radar used here is in the centimeter range, which is many many times longer than the visible light our eyes perceive in the nanometer range. In short, we can’t see these radar light waves, but the radar sensors can.   Essentially, these sensors emit radiation (a.k.a. light) at a specific frequency and measure the amount of light returned to the sensor. In general, smoother surfaces reflect more light away (like looking at a mirror from the side) and rougher surfaces reflect more light back towards the sensor.   The exact interaction of the light with sea ice and open water is more complex, but in general smoother surfaces like calm open water and smoother sea ice show up as darker in the radar image, whereas windy water, sea ice ridges, and a variety of other rougher surfaces from sea ice show up as lighter in the radar image.   BIG DISCLAIMER: The interpretation of sea ice from radar images is very difficult, and I am not an expert. Always use extreme caution when venturing out on the ice pack. This is an excellent guide to sea ice interpretation from radar, and here’s an image showing the general relationship and myriad complications of measuring sea ice extent and type from radar imagery:      Image from here, based on modification of Johannessen et al. (1997)    The basic thing to keep in mind is:      smoother = low radar backscatter = darker in radar image   rougher = high radar backscatter = lighter in radar image   On to the good stuff   With that out of the way, let’s look at some pretty pictures.   What does it actually look like?   Before we get into the weird world of radar, how does it look with our own eyes on the coast off of Nome right now?      That’s a picture from today. You can see the relatively smoother, stable, older ice near-shore and then some ridges of rafted ice about one or two kilometers out in the distance.   Sentinel-1 Satellite Radar   The satellite data I want to check out is from the Sentinel-1 satellite pair. These are radar (technically a synthetic aperture radar, or SAR) satellites that have been up in orbit since 2015. The data is free to the public. Unfortunately, one of these pair was recently decommissioned, so rather than getting an image every day or two in the arctic, we only get one every four or five days.   Here’s a timelapse of sea ice from a few days when the satellite was passing over Nome in the past few weeks:      (I made the Sentinel timelapse images throughout this post using Google Earth Engine.)   For reference, the cross in the top center of the timelapse is the airport (each arm of the cross is a runway), and the town of Nome is the bright (white) patch just east of the airport. The Bering Sea is everything south of the east-west oriented coastline and the port is the white arm sticking out into the sea off of the coast.   And look! The runways are a smooth surface, and they appear dark in the radar image (low backscatter), whereas town is very rough (lots of buildings) and thus appears very light in the radar image (high backscatter).   If we keep in mind that general rule, then we can interpret the whiter pixels in the Bering Sea as rough sea ice. Although it could also be rough, windy seas, but the winds have not been especially strong these first few weeks of April. On the other hand, it has been very, very cold (record breaking for April in fact). So white = rough sea ice, and that makes the darker black pixels open water. Pixels in between black and white might be some of that smoother ice like we saw in my photo just off the coast.   Look how much it’s changing a few kilometers off of the coast in the two weeks shown in this timelapse! Neat! (And tread carefully out there.)   What about the other polarization?   Okay, true, I only showed the VV (vertical-vertical) polarized timelapse, but we can also look at the cross-polarized (vertical-horizontal, or VH) image from Sentinel-1. There are differences in the sea-ice interaction between these two modes of sensor operation, but in general the results are similar, showing near-shore consistent ice pack and a lot of movement nearby off shore:      I also really like how the radar images show the smoother (darker pixels) on the land north of the coastline, where the tundra is covered in several meters of mostly smooth snow. The lighter pixels on the land are a mixture of human infrastructure (buildings, roads, mining operations) and natural, rough features occurring in river valleys (willows poking up through the snow).   Radar From Port   It looks like something happened between April 5th and 10th that dramatically changed the ice extent and opened up a large amount of open water a few kilometers off of the coast. Unfortunately, we only have a limited number of satellite scenes, so we can’t fill those gaps from the current data.   But, how do the ground-based port radar measurements represent this time period?   A couple of notes in comparison to the Sentinel-1 satellite: the ground based radar has a much higher spatial resolution (each pixel shows a smaller area), it has a much higher temporal resolution (the timelapse is composed of one image every four minutes), and the radar wavelength is shorter (by about half).   Regardless of these differences, the same general rule of darker = smoother and lighter = rougher hold for interpreting this video timelapse courtesy of UAF (Press the play button to watch it):           Sea ice radar timelapse from April 7th, 8th, 9th from here    Clearly in the early morning on April 8th, a big raft of ice broke off and began floating away, opening things up like we saw in the satellite! Thanks, port radar!   There are a lot of differences compared to the satellite imagery (also in the extent of coverage, where the ground measurement is only showing an eight kilometer radius from the pier), but clearly the data is complementary and show generally similar trends:   Nearby the shore we currently have pretty stable ice, but a few kilometers out we have frequent changes in the ice pack. Let’s see how long into the spring this ice lasts.   Hold up, why not optical images?   Radar is neat and all, but we also have a bunch of optical satellites orbiting the earth, like Landsat and Sentinel-2. Why not just use these easy-to-interpret pictures from space?   Well, in this case, we actually do have some nice Sentinel-2 imagery to corroborate the radar data (ice becoming open water):      But, the first image in this timelapse from March 26th shows the issue with optical data: it doesn’t penetrate cloud cover. And cloudy days are pretty frequent, making it difficult to get the consistent measurement of sea ice that radar offers with its ability to “see through” any cloud cover.   ","categories": [],
        "tags": ["geemap","sentinel","sar","nome","sea-ice"],
        "url": "/sea-ice-radar/",
        "teaser": "/assets/images/sea-ice-office.jpg"
      },{
    "title": "Page Not Found",
    "excerpt":"Sorry, but the page you were trying to view does not exist — perhaps you can try searching for it below.       ","url": "http://localhost:4000/404.html"
  },{
    "title": "Earth Scientist, Developer, Educator",
    "excerpt":"   The poles of the earth have wandered. The equator has apparently moved. The continents, perched on their plates, are thought to have been carried so very far and to be going in so many directions that it seems an act of almost pure hubris to assert that some landmark of our world is fixed at 73 degrees 57 minutes and 53 seconds west longitude and 40 degrees 51 minutes and 14 seconds north latitude–a temporary description, at any rate, as if for a boat on the sea. - John McPhee    Hey, I’m Ben. Formerly a scientist within the Remote Sensing and Earth Surface Processes group at the University of Potsdam in Germany. Now working at the University of Washington in the Terrain Analysis and Cryosphere Observation Lab (With a few years in sub-arctic Nome, Alaska along the way.)   I have my feet in a few worlds. Aside from my science jobs, I also work for First Draft, developing a web application development course for beginner programmers. This has been my primary focus for the past few years.   I’m open to consulting, and have run workshops for GIS mapping. My inbox is open if you want to get in touch about workshops, projects, or anything else!     My background is in geology, including field observations and mapping, but I specialized in remote sensing and geomorphology. I use satellites, drones, handheld cameras, geodetic measurements, and a lot of computer coding to investigate the driving forces and resulting shape of the earth’s surface.   Lately, my focus has shifted more towards education, both in the context of developing accessible and inclusive learning experiences for beginner programmers, and in developing open source software for geoscience.   You can check out my GitHub for code snippets and inspiration. I enjoy science communication and get my fix via instagram.   On this site, my research page contains some of the science I’ve done in the past, with a few links to the publications that came out of it. You can also find my current CV, and some regrettably infrequent blog posts.         Quebrada del Toro in northwestern Argentina, my PhD study area.    ","url": "http://localhost:4000/"
  },{
    "title": null,
    "excerpt":"                 CV Template                                  📄 Print CV / Save as PDF                                                         Dr. [Your Name]                  Head of Operations | Former Research Scientist                                      [your.email@company.com]                     [+1 (555) 123-4567]                     [City, State]                     [linkedin.com/in/yourname]                                                                               Professional Summary                                       Results-driven operations leader with a unique background bridging rigorous scientific research and technology education. Currently serving as Head of Operations at [Company Name], where I leverage analytical thinking and systematic approaches developed through [X] years of academic research to scale operations for a fast-growing coding education platform. Proven track record of translating complex technical concepts into accessible learning experiences while building efficient systems that support rapid organizational growth.                                                             Core Experience                                       + Add Experience                                                                                                                              Head of Operations                             [Company Name] - Education Technology Startup                                                  [Start Date] - Present                                                                   Leading end-to-end operations for a rapidly scaling coding education platform serving [X] students globally. Drive strategic initiatives that improve operational efficiency, student outcomes, and organizational growth.                                                      Designed and implemented scalable operational frameworks that supported [X]% growth in student enrollment                             Built cross-functional processes connecting curriculum development, student success, and business operations                             Led data-driven decision making using analytical skills from research background to optimize key metrics                             Managed vendor relationships and strategic partnerships to enhance platform capabilities                             Developed quality assurance processes ensuring consistent educational delivery across all programs                                                                                            Delete                                                                                  Academic & Research Background                                       + Add Research                                                                                                                              [Previous Research Position]                             [Institution/Company Name]                                                  [Start Date] - [End Date]                                                                   [Brief description of your research focus and key contributions]                                                      [Key research achievement or publication]                             [Method or technique you developed/mastered]                             [Impact or application of your work]                             [Collaboration or leadership experience]                                                                                            Delete                                                                                                                                   [Additional Research Role]                             [Institution Name]                                                  [Start Date] - [End Date]                                                                   [Description of role and achievements]                                                      [Key accomplishment]                             [Technical skills demonstrated]                             [Research outcomes]                                                                                            Delete                                                                                  Education                                       + Add Education                                                                                                                              [Ph.D./Master's] in [Field]                             [University Name]                                                  [Graduation Year]                                                                   Dissertation: \"[Thesis Title]\"                         Advisor: [Advisor Name]                         [Notable achievements, awards, or relevant coursework]                                                                   Delete                                                                                                                                   [Bachelor's Degree] in [Field]                             [University Name]                                                  [Graduation Year]                                                                   [Honors, GPA if notable, relevant achievements]                                                                   Delete                                                                                  Core Competencies                                       + Add Skill Category                                                                                Operations & Strategy                                                       Process Design                             Scale Operations                             Strategic Planning                             Quality Assurance                             Vendor Management                                                                               + Skill                             Delete                                                                                            Technical & Research                                                       Data Analysis                             Statistical Modeling                             [Your Research Tools]                             Research Design                             Technical Writing                                                                               + Skill                             Delete                                                                                            Leadership & Communication                                                       Cross-functional Teams                             Stakeholder Management                             Public Speaking                             Curriculum Development                             Mentoring                                                                               + Skill                             Delete                                                                                            Technology & Tools                                                       [Programming Languages]                             [Lab Equipment]                             Project Management                             [Software Tools]                             Database Management                                                                               + Skill                             Delete                                                                                                           Selected Publications & Research                                       + Add Publication                                                                        [Publication Title]                                              [Author names]. [Journal Name], [Year]. [DOI or link if applicable]                                                                   Delete                                                                             [Publication Title]                                              [Author names]. [Journal Name], [Year]. [DOI or link if applicable]                                                                   Delete                                                                             [Conference Presentation or Additional Publication]                                              [Details about presentation/publication]                                                                   Delete                                                                                  Awards & Recognition                                       + Add Achievement                                                                                [Award/Recognition Name]                         [Year] - [Brief description of achievement]                                                      Delete                                                                                            [Grant or Fellowship]                         [Year] - [Amount if appropriate] - [Granting organization]                                                      Delete                                                                                            [Professional Recognition]                         [Year] - [Description of recognition]                                                      Delete                                                                                                   ","url": "http://localhost:4000/cv-builder/"
  },{
    "title": "CV",
    "excerpt":"  Here is a link to my current full CV, written in a detailed, academic format. My professional highlights are below.     Education      PhD, Remote Sensing, University of Potsdam, Germany, February 2020   MSc, Geology, University of Potsdam, Germany, November 2016   BA, Earth &amp; Environmental Sciences, Wesleyan University, USA, May 2013   Experience      Always: Consulting and workshops for GIS mapping and geoscientific data analysis   November 2022 - present: Educator and Developer, First Draft, LLC   March 2023 - present: Teaching Assistant, University of Chicago, USA   June 2023 - present: Research Scientist, University of Washington, USA   November 2022 - April 2024: Living and working in Nome, Alaska   June 2020 - October 2022: Post-Doctoral Researcher and Instructor, University of Potsdam, Germany   2017 - 2020: PhD Candidate and Teaching Assistant, University of Potsdam, Germany   March 2015 - 2019: Fieldwork in the Eastern Central Andes for master’s and doctoral theses   Summer 2012: Keck Consortium funded fieldwork for Colorado Front Range bachelor’s thesis   Spring 2012: Geochemistry lab technician at Wesleyan University   Summer 2011: NSF funded research intern at Lamont-Doherty Earth Observatory   Skills   Programming      Python, Bash scripting (5+ years): Geospatial and statistical tasks   Ruby, Rails, Django, JavaScript, SQL, HTML, CSS (3+ years): Full-stack web application development   Software      GIS and Remote Sensing: QGIS, ArcGIS, GMT, GDAL/OGR, ENVI, PCI-Geomatica, SNAP   Point Clouds: Agisoft Metashape, Pix4D, CloudCompare, LAStools, PDAL   Topographic Analysis: TopoToolbox (Matlab), LSDTopoTools (Command Line)   Other: Adobe Illustrator, LaTeX, markdown, pandoc   Methods      Technical scientific writing and communication to wider audiences   Quantitative statistical analysis of large environmental datasets, including principles of machine learning   Optical and radar satellite data management, image processing, and analysis   Collection and processing of precise geodetic measurements   Structure-from-Motion processing of photo surveys to generate point clouds and digital surface models  ","url": "http://localhost:4000/cv/"
  },{
    "title": "My Blog - Science, Coding, etc.",
    "excerpt":"\r \r \r \r   \r   \r     \r       \r         Exploring Nome's Sea Ice with Satellite and Ground Radar\r       \r      \r \r     12 Apr 2023\r \r     \r     I recently found out about UAF’s sea-ice radar...\r     \r   \r   \r   \r     \r       \r         Running dockerized google earth engine (GEE) map\r       \r      \r \r     25 Mar 2023\r \r     \r     The steps I took to open and run a dockerized...\r     \r   \r   \r   \r     \r       \r         Notes on using git\r       \r      \r \r     18 Mar 2023\r \r     \r     I’m currently going through the extremely helpful thoughtbot...\r     \r   \r   \r   \r     \r       \r         Git merging main into an in-progress feature-branch\r       \r      \r \r     17 Mar 2023\r \r     \r     In a collaborative (or solo) git project I’m...\r     \r   \r   \r   \r     \r       \r         Setting up Ruby on Rails with PostgreSQL\r       \r      \r \r     15 Feb 2023\r \r     \r     I just spent a long time getting my Windows machine...\r     \r   \r   \r   \r     \r       \r         Custom JavaScript in Bookdown\r       \r      \r \r     07 Feb 2023\r \r     \r     I wanted some custom behavior in my bookdown project. Specifically,...\r     \r   \r   \r   \r     \r       \r         Writing a textbook with bookdown\r       \r      \r \r     11 Jan 2023\r \r     \r     I am working on an outline for a textbook for...\r     \r   \r   \r   \r     \r       \r         Adding a `sitemap.xml` for SEO\r       \r      \r \r     04 Jan 2023\r \r     \r     SEO stands for Search Engine Optimization. I want to improve...\r     \r   \r   \r   \r     \r       \r         First Post!\r       \r      \r \r     03 Jan 2023\r \r     \r     This blog is going to be a collection of notes...\r     \r   \r   \r   \r     \r       \r         Getting started with a jekyll, minimal mistakes, and github pages\r       \r      \r \r     03 Jan 2023\r \r     \r     This site was built with jekyll. In my...\r     \r   \r   \r   \r     \r       \r         Deploying github page to own domain\r       \r      \r \r     03 Jan 2023\r \r     \r     I wanted something a little more professional than \r   \r   \r     \r       \r         Changing in-line code color\r       \r      \r \r     03 Jan 2023\r \r     \r     I was getting annoyed with the difficulty of seeing the...\r     \r   \r   \r \r \r ","url": "http://localhost:4000/blog/"
  },{
    "title": null,
    "excerpt":"var idx = lunr(function () {   this.field('title')   this.field('excerpt')   this.field('categories')   this.field('tags')   this.ref('id')    this.pipeline.remove(lunr.trimmer)    for (var item in store) {     this.add({       title: store[item].title,       excerpt: store[item].excerpt,       categories: store[item].categories,       tags: store[item].tags,       id: item     })   } });  $(document).ready(function() {   $('input#search').on('keyup', function () {     var resultdiv = $('#results');     var query = $(this).val().toLowerCase();     var result =       idx.query(function (q) {         query.split(lunr.tokenizer.separator).forEach(function (term) {           q.term(term, { boost: 100 })           if(query.lastIndexOf(\" \") != query.length-1){             q.term(term, {  usePipeline: false, wildcard: lunr.Query.wildcard.TRAILING, boost: 10 })           }           if (term != \"\"){             q.term(term, {  usePipeline: false, editDistance: 1, boost: 1 })           }         })       });     resultdiv.empty();     resultdiv.prepend(''+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-en.js"
  },{
    "title": null,
    "excerpt":"step1list = new Array(); step1list[\"ΦΑΓΙΑ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΟΥ\"] = \"ΦΑ\"; step1list[\"ΦΑΓΙΩΝ\"] = \"ΦΑ\"; step1list[\"ΣΚΑΓΙΑ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΟΥ\"] = \"ΣΚΑ\"; step1list[\"ΣΚΑΓΙΩΝ\"] = \"ΣΚΑ\"; step1list[\"ΟΛΟΓΙΟΥ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΑ\"] = \"ΟΛΟ\"; step1list[\"ΟΛΟΓΙΩΝ\"] = \"ΟΛΟ\"; step1list[\"ΣΟΓΙΟΥ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΑ\"] = \"ΣΟ\"; step1list[\"ΣΟΓΙΩΝ\"] = \"ΣΟ\"; step1list[\"ΤΑΤΟΓΙΑ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΟΥ\"] = \"ΤΑΤΟ\"; step1list[\"ΤΑΤΟΓΙΩΝ\"] = \"ΤΑΤΟ\"; step1list[\"ΚΡΕΑΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΟΣ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΑ\"] = \"ΚΡΕ\"; step1list[\"ΚΡΕΑΤΩΝ\"] = \"ΚΡΕ\"; step1list[\"ΠΕΡΑΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΟΣ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΑ\"] = \"ΠΕΡ\"; step1list[\"ΠΕΡΑΤΩΝ\"] = \"ΠΕΡ\"; step1list[\"ΤΕΡΑΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΟΣ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΑ\"] = \"ΤΕΡ\"; step1list[\"ΤΕΡΑΤΩΝ\"] = \"ΤΕΡ\"; step1list[\"ΦΩΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΟΣ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΑ\"] = \"ΦΩ\"; step1list[\"ΦΩΤΩΝ\"] = \"ΦΩ\"; step1list[\"ΚΑΘΕΣΤΩΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΟΣ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΑ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΚΑΘΕΣΤΩΤΩΝ\"] = \"ΚΑΘΕΣΤ\"; step1list[\"ΓΕΓΟΝΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΟΣ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΑ\"] = \"ΓΕΓΟΝ\"; step1list[\"ΓΕΓΟΝΟΤΩΝ\"] = \"ΓΕΓΟΝ\";  v = \"[ΑΕΗΙΟΥΩ]\"; v2 = \"[ΑΕΗΙΟΩ]\"  function stemWord(w) {   var stem;   var suffix;   var firstch;   var origword = w;   test1 = new Boolean(true);    if(w.length '+result.length+' Result(s) found ');     for (var item in result) {       var ref = result[item].ref;       if(store[ref].teaser){         var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+                 ''+               ''+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       else{     \t  var searchitem =           ''+             ''+               ''+                 ''+store[ref].title+''+               ' '+               ''+store[ref].excerpt.split(\" \").splice(0,20).join(\" \")+'... '+             ''+           '';       }       resultdiv.append(searchitem);     }   }); }); ","url": "http://localhost:4000/assets/js/lunr/lunr-gr.js"
  },{
    "title": null,
    "excerpt":"var store = [   {%- for c in site.collections -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}     {%- assign docs = c.docs | where_exp:'doc','doc.search != false' -%}     {%- for doc in docs -%}       {%- if doc.header.teaser -%}         {%- capture teaser -%}{{ doc.header.teaser }}{%- endcapture -%}       {%- else -%}         {%- assign teaser = site.teaser -%}       {%- endif -%}       {         \"title\": {{ doc.title | jsonify }},         \"excerpt\":           {%- if site.search_full_content == true -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | jsonify }},           {%- else -%}             {{ doc.content | newline_to_br |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \" |               replace:\" \", \" \"|             strip_html | strip_newlines | truncatewords: 50 | jsonify }},           {%- endif -%}         \"categories\": {{ doc.categories | jsonify }},         \"tags\": {{ doc.tags | jsonify }},         \"url\": {{ doc.url | relative_url | jsonify }},         \"teaser\": {{ teaser | relative_url | jsonify }}       }{%- unless forloop.last and l -%},{%- endunless -%}     {%- endfor -%}   {%- endfor -%}{%- if site.lunr.search_within_pages -%},   {%- assign pages = site.pages | where_exp:'doc','doc.search != false' -%}   {%- for doc in pages -%}     {%- if forloop.last -%}       {%- assign l = true -%}     {%- endif -%}   {     \"title\": {{ doc.title | jsonify }},     \"excerpt\":         {%- if site.search_full_content == true -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | jsonify }},         {%- else -%}           {{ doc.content | newline_to_br |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \" |             replace:\" \", \" \"|           strip_html | strip_newlines | truncatewords: 50 | jsonify }},         {%- endif -%}       \"url\": {{ doc.url | absolute_url | jsonify }}   }{%- unless forloop.last and l -%},{%- endunless -%}   {%- endfor -%} {%- endif -%}] ","url": "http://localhost:4000/assets/js/lunr/lunr-store.js"
  },{
    "title": null,
    "excerpt":"                              Form Demo                   &lt;form action=\"https://www.example.com/users/sign_up\"&gt;    &lt;label for=\"email_field\"&gt;My email:&lt;/label&gt;   &lt;input id=\"email_field\" type=\"text\" name=\"my_email\"&gt;    &lt;label for=\"password_field\"&gt;My password:&lt;/label&gt;   &lt;input id=\"password_field\" type=\"password\" name=\"my_password\"&gt;    &lt;button&gt;Pretend to sign up&lt;/button&gt; &lt;/form&gt;                                                              My email:                                                                                                        My password:                                                                            Pretend to sign up                                                           ","url": "http://localhost:4000/password-form-demo/"
  },{
    "title": "Publications",
    "excerpt":" \r See also: [Google Scholar Profile](https://scholar.google.de/citations?user=EacO3GQAAAAJ&hl=en){:target=\"_blank\"} and [ORCID](https://orcid.org/0000-0001-8504-8115){:target=\"_blank\"}\r \r \r  \r If you can't access a paper, email me and I'll send you a personal copy. \r \r \r \r **Purinton, B.**, Mueting, A. and Bookhagen, B.: Image Texture as Quality Indicator for Optical DEM Generation: Geomorphic Applications in the Arid Central Andes, _Remote Sensing_, [https://doi.org/10.3390/rs15010085](https://www.mdpi.com/2072-4292/15/1/85){:target=\"_blank\"}, 2023.\r \r **Purinton, B.** and Bookhagen, B.: Beyond vertical point accuracy: Assessing inter-pixel consistency in 30 m global DEMs for the arid Central Andes, _Frontiers in Earth Science_, [https://doi.org/10.3389/feart.2021.758606](https://www.frontiersin.org/articles/10.3389/feart.2021.758606){:target=\"_blank\"}, 2021.\r \r **Purinton, B.** and Bookhagen, B.: Tracking Downstream Variability in Large Grain-Size Distributions in the South-Central Andes, _Journal of Geophysical Research: Earth Surface_, 126(8):e2021JF006260, [https://doi.org/10.1029/2021JF006260](https://doi.org/10.1029/2021JF006260){:target=\"_blank\"}, 2021.\r \r **Purinton, B.** and Bookhagen, B.: Multiband (X, C, L) radar amplitude analysis for a mixed sand- and gravel-bed river in the eastern Central Andes, _Remote Sensing of Environment_, 246:111799, [https://doi.org/10.1016/j.rse.2020.111799](https://doi.org/10.1016/j.rse.2020.111799){:target=\"_blank\"}, 2020.\r \r **Purinton, B.** and Bookhagen, B.: Introducing PebbleCounts: A grain-sizing tool for photo surveys of dynamic gravel-bed rivers, _Earth Surface Dynamics_, 7, 859–877, [https://doi.org/10.5194/esurf-7-859-2019](https://doi.org/10.5194/esurf-7-859-2019){:target=\"_blank\"}, 2019.\r \r **Purinton, B.** and Bookhagen, B.: Measuring decadal vertical land-level changes from SRTM-C (2000) and TanDEM-X (~2015) in the south-central Andes, _Earth Surface Dynamics_, 6, 971-987, [https://doi.org/10.5194/esurf-6-971-2018](https://doi.org/10.5194/esurf-6-971-2018){:target=\"_blank\"}, 2018.\r \r **Purinton, B.** and Bookhagen, B.: Validation of digital elevation models (DEMs) and comparison of geomorphic metrics on the southern Central Andean Plateau, _Earth Surface Dynamics_, 5, 211-237, [https://doi.org/10.5194/esurf-5-211-2017](https://doi.org/10.5194/esurf-5-211-2017){:target=\"_blank\"}, 2017.\r ","url": "http://localhost:4000/publications/"
  },{
    "title": "Research",
    "excerpt":"## Landscape recovery following wildfire\r \r For my B.A. at Wesleyan University I had the opportunity to work on a [Keck Consortium](https://keckgeology.org/){:target=\"_blank\"} research project in the Colorado Front Range. In summer 2012, following geology field camp school in Wyoming, Idaho, and Montana, I moved into a cabin at Colorado University's [Mountain Research Station](https://www.colorado.edu/mrs/){:target=\"_blank\"} to begin four weeks studying the impact of a 2010 wildfire in Fourmile Canyon outside Boulder, Colorado. I studied the relaxation time of extreme erosion events brought on by moderate rainfall following the fire, and also the influence of historical mine tailings piles on the geochemistry of the sediment. Here's my [thesis](/assets/documents/BP_ba_thesis.pdf){:target=\"_blank\"}.\r \r \r \r \r \r A typical Wyoming view during field school.\r  \r \r \r \r Return of grasses and shrubs among severely burned trees in Fourmile Canyon.\r  \r \r ---\r \r ## Measuring topography from space\r \r ### Validation of DEMs\r \r In this work for my masters I validated the elevation accuracy and geomorphic potential of near-global spaceborne Digital Elevation Models (DEMs), including SRTM-C, TanDEM-X, ALOS World 3D, and ASTER GDEM2. I also worked on photogrammetric DEM production with raw optical scenes like ASTER L1A, RapidEye, SPOT6, and ALOS PRISM, as well as interferometric DEM production from TerraSAR-X/TanDEM-X CoSSC scenes. Validation was carried out with a huge dataset of differential GNSS measurements collected over three field seasons in northwestern Argentina. Here's the [publication in Earth Surface Dynamics](https://www.earth-surf-dynam.net/5/211/2017/){:target=\"_blank\"} that came out of it. More recently, we also improved on validation using sparse benchmark data through Fourier analysis in the frequency domain to examine the inter-pixel consistency of spaceborne DEMs. You can read about that in [Frontiers](https://www.frontiersin.org/articles/10.3389/feart.2021.758606){:target=\"_blank\"}.\r \r \r \r Collecting differential GPS measurements from a salar following a rare rain event on the arid and remote Puna Plateau in Argentina.\r  \r \r ### Measuring decadal land-level changes using DEMs\r \r Akin to studies of DEM differencing for measuring snow and ice change, I applied careful correction and differencing to the TanDEM-X (collected 2011-2015) and SRTM-C (collected 2000) DEMs to measure ~15 years of land-level changes in some large catchments draining the eastern flank of the Andes in northwestern Argentina. Due to the small magnitude of most land-level changes, the 15-year separation between the DEMs only allowed for sparse automated geophysical results. These vertical change signals were defined outside of remaining vertical uncertainty, primarily caused by the lower quality SRTM-C DEM, despite careful bias correction procedures developed. We were able to gain insight into riverbed aggradation and incision in the Río Toro and Río Grande, particulalry in downstream sections, which are steeper (due to knickpoints), wetter, and modified by anthropogenic activity. Here's the [publication in Earth Surface Dynamics](https://www.earth-surf-dynam.net/6/971/2018/){:target=\"_blank\"} that came out of it.\r \r \r \r   \r \r \r \r Lower Río Toro shot on a DJI Mavic Pro drone. Note the big gravel piles being formed to prevent rapid channel-bed aggradation.\r  \r \r ---\r \r \r ## River surveys, pebble counting, and downstream changes in sediment character from SAR\r \r Moving from the scale of entire catchments using meter-scale spaceborne DEMs, I collected 1,000-5,000 m2 river-bed surveys at sub-cm scales. The surveys, amounting to over 50,000 photos, were collected using a camera-on-mast setup (see the videos below). We also flew some drone surveys to supplement these data. Point clouds, micro-DEMs, and orthomosaics were generated by structure-from-motion and multi-view stereo (SfM-MVS) processing. Following dissatisfying results from existing tools, our intention to count the grain-size distributions was realized in our Python-based software [PebbleCounts](https://github.com/UP-RS-ESP/PebbleCounts){:target=\"_blank\"}, with the accompanying paper [in Earth Surface Dynamics](https://www.earth-surf-dynam.net/7/859/2019/){:target=\"_blank\"}.\r \r Recently, we applied PebbleCounts to examine downstream fining in the Quebrada del Toro in northwestern Argentina, and we found some interesting variability at the coarse tail of the distribution. Read about that in [JGR](https://doi.org/10.1029/2021JF006260){:target=\"_blank\"}.\r \r We also explored the use of spaceborne Synthetic Aperature Radar (SAR) measurements of surface roughness from channel beds using satellites like TerraSAR-X, Sentinel-1, and ALOS PALSAR. These SAR data can be used mainly for discerning gravel and sand transitions over huge areas, but it is hard to look at absolute grain sizes in the channel...see the publication [in Remote Sensing of Environment](https://doi.org/10.1016/j.rse.2020.111799){:target=\"_blank\"}.\r \r \r \r   \r \r \r \r Surveying in the upper Quebrada del Toro Catchment in calm conditions.\r  \r \r \r \r \r   \r \r \r \r Surveying in the upper Quebrada del Toro Catchment in high winds on a Quaternary fluvial terrace.\r  \r ","url": "http://localhost:4000/research/"
  },{
    "title": "Posts by Tag",
    "excerpt":"","url": "http://localhost:4000/tags/"
  },{
    "title": null,
    "excerpt":"{% if page.xsl %}{% endif %}Jekyll{{ site.time | date_to_xmlschema }}{{ page.url | absolute_url | xml_escape }}{% assign title = site.title | default: site.name %}{% if page.collection != \"posts\" %}{% assign collection = page.collection | capitalize %}{% assign title = title | append: \" | \" | append: collection %}{% endif %}{% if page.category %}{% assign category = page.category | capitalize %}{% assign title = title | append: \" | \" | append: category %}{% endif %}{% if title %}{{ title | smartify | xml_escape }}{% endif %}{% if site.description %}{{ site.description | xml_escape }}{% endif %}{% if site.author %}{{ site.author.name | default: site.author | xml_escape }}{% if site.author.email %}{{ site.author.email | xml_escape }}{% endif %}{% if site.author.uri %}{{ site.author.uri | xml_escape }}{% endif %}{% endif %}{% if page.tags %}{% assign posts = site.tags[page.tags] %}{% else %}{% assign posts = site[page.collection] %}{% endif %}{% if page.category %}{% assign posts = posts | where: \"categories\", page.category %}{% endif %}{% unless site.show_drafts %}{% assign posts = posts | where_exp: \"post\", \"post.draft != true\" %}{% endunless %}{% assign posts = posts | sort: \"date\" | reverse %}{% assign posts_limit = site.feed.posts_limit | default: 10 %}{% for post in posts limit: posts_limit %}{% assign post_title = post.title | smartify | strip_html | normalize_whitespace | xml_escape %}{{ post_title }}{{ post.date | date_to_xmlschema }}{{ post.last_modified_at | default: post.date | date_to_xmlschema }}{{ post.id | absolute_url | xml_escape }}{% assign excerpt_only = post.feed.excerpt_only | default: site.feed.excerpt_only %}{% unless excerpt_only %}{% endunless %}{% assign post_author = post.author | default: post.authors[0] | default: site.author %}{% assign post_author = site.data.authors[post_author] | default: post_author %}{% assign post_author_email = post_author.email | default: nil %}{% assign post_author_uri = post_author.uri | default: nil %}{% assign post_author_name = post_author.name | default: post_author %}{{ post_author_name | default: \"\" | xml_escape }}{% if post_author_email %}{{ post_author_email | xml_escape }}{% endif %}{% if post_author_uri %}{{ post_author_uri | xml_escape }}{% endif %}{% if post.category %}{% elsif post.categories %}{% for category in post.categories %}{% endfor %}{% endif %}{% for tag in post.tags %}{% endfor %}{% assign post_summary = post.description | default: post.excerpt %}{% if post_summary and post_summary != empty %}{% endif %}{% assign post_image = post.image.path | default: post.image %}{% if post_image %}{% unless post_image contains \"://\" %}{% assign post_image = post_image | absolute_url %}{% endunless %}{% endif %}{% endfor %}","url": "http://localhost:4000/feed.xml"
  },{
    "title": null,
    "excerpt":" {% if page.xsl %} {% endif %} {% assign collections = site.collections | where_exp:'collection','collection.output != false' %}{% for collection in collections %}{% assign docs = collection.docs | where_exp:'doc','doc.sitemap != false' %}{% for doc in docs %} {{ doc.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if doc.last_modified_at or doc.date %}{{ doc.last_modified_at | default: doc.date | date_to_xmlschema }} {% endif %} {% endfor %}{% endfor %}{% assign pages = site.html_pages | where_exp:'doc','doc.sitemap != false' | where_exp:'doc','doc.url != \"/404.html\"' %}{% for page in pages %} {{ page.url | replace:'/index.html','/' | absolute_url | xml_escape }} {% if page.last_modified_at %}{{ page.last_modified_at | date_to_xmlschema }} {% endif %} {% endfor %}{% assign static_files = page.static_files | where_exp:'page','page.sitemap != false' | where_exp:'page','page.name != \"404.html\"' %}{% for file in static_files %} {{ file.path | replace:'/index.html','/' | absolute_url | xml_escape }} {{ file.modified_time | date_to_xmlschema }}  {% endfor %} ","url": "http://localhost:4000/sitemap.xml"
  },{
    "title": null,
    "excerpt":"Sitemap: {{ \"sitemap.xml\" | absolute_url }} ","url": "http://localhost:4000/robots.txt"
  }]
